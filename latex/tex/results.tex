% Tulokset / results
\section{Experiments}

% is X? test X. proof: X is ?.

\subsection{Synchronization issues}

Synchronizing the image sources temporally is important for reconstruction algorithms to work correctly.
A synchronized system captures images from all cameras with a reasonably small time difference, so that subject movement does not introduce additional errors.

\subsubsection{Still capture}

Static subjects do not move or change shape.
However, still imagery of e.g. a human head needs at least semi-accurate syncing, because standing still enough is not possible for longer periods (seconds).
Even though the facial pose would not change, the wrong position of the head in a frame would introduce error.
Similarly, capturing e.g. a still image of a moving cloth needs proper synchronization.

For a completely static subject, the cameras can even be shot separately, each using its own integrated flash unit, to minimize shadows for each viewpoint.
When the light direction is almost the same as the camera's view direction, no shadows are possible.

Still capture using the wired remote trigger was analyzed with a moving object.
%A spinning disk with a dial was captured and the images were compared.
%Sample data is shown in figure \ref{fig:spindisk}

\subsubsection{Video capture}

% intro

The EOS 700D does not support an external sync signal for video recording, but relies on an internal clock to sync the frames within a single video file.
It can be safely assumed that the clock speed has little variation, and thus, little drift and jitter occur in the videos.
Recording offset remains an issue;
processing time of each camera from the shutter button to start of first frame varies because of unknown reasons; even though a wireless remote control of the camera manufacturer would be used for starting several cameras at once, the offset is very noticeable.

% what was done

A test was carried out to find variations in the recording offset.
Three cameras were triggered with a Canon RC-6 wireless remote control.
The remote sends a short infrared pulse to a sensor in the front of a camera when a key is pressed.
The camera was set in video mode and it was configured to use the remote release.
%The exposure time was set to minimum, 1/4000 s, to maximise the rolling shutter effect.
%In this mode, the sensor rows are exposed one by one in succession.
When all cameras were recording, a flash unit (Canon Speedlite 420EX) was aimed at the lenses and fired manually several times.

% what happens

A direct light pulse of a flash unit shows up as an over-exposed, completely white video frame, if the whole frame was exposing the image during the light pulse.
Sometimes only part of the frame would be exposed to the flash light as a rolling shutter effect.
Because the CMOS sensor is cleared and read line by line, the time of the flash can be deduced from the first bright line in the image.
The rows are zeroed from light in succession in the same way as a mechanical focal plane shutter moves and exposed until read.
Thus, the flash light's starting time in the video's internal clock is the frame's start time plus the duration that it takes for the rows before the overexposed one to show up:

\begin{align} \label{timingcalib}
t_{Cl} &= t_{Ci} + t_{Cf} \\
&= i_C * F + f_C / R * F \\
&= (i_C + f_C / r * c) * F
\end{align}

where $C$ is a camera identifier, $t_{Cl}$ is camera-relative time for the light pulse, $t_{Ci}$ is the starting time for the frame where the light shows up, and $t_{Cf}$ is the time inside the particular frame.
The frame index $i$ marks the number of the frame in the video file, starts from 0, and there is $F$ = $1/\text{fps}$ time between the frames.
$f_C$ is the first exposed row, $R = r / c$ is the time for a single row to expose, $r$ is the vertical frame resolution and $c$ is a correction coefficient describing the relation of exposure time and frame duration ($0.5$ in this case, referring to 180 degree shutter).
Offset between two cameras $A$ and $B$ is then $t_{Al} - t_{Bl}$.
The timings are depicted in figure \ref{fig:flashtimeline} as a timeline with matching symbols.

For perfectly synchronized videos, the frames would be identical, and time difference calculated with eq. \ref{timingcalib} for each frame would be zero as $i_C$ and $f_C$ would be constants for all $C$.
A typical measurement result is shown in figure \ref{fig:flashtiming}.
Audio recorded by the cameras' internal microphones at the same event is shown in figure \ref{fig:flashaudio}, and the order of flash pulses in the three frames can be verified from the order of the sound emitted by the flash.

\simplegfx{h}{1.0\textwidth}{flashtimeline}
{Illustration on timing calibration of two cameras using a flash light.
The darkened areas represent the exposure times and the red band represents the light pulse.
In this example, $c = 0.5$.
}

\simplefig{p}{%
\setlength\fboxsep{0pt}
\setlength\fboxrule{1pt}
\fbox{\includegraphics[width=0.3\textwidth]{flashtiming-a}}
\fbox{\includegraphics[width=0.3\textwidth]{flashtiming-b}}
\fbox{\includegraphics[width=0.3\textwidth]{flashtiming-c}}
}{fig:flashtiming}
{Video frames of three cameras started with the same remote, with the same frame index.
The time for the flash unit to reach maximum brightness is almost instant.
The start of the light pulse can be seen in the leftmost and rightmost frames.
In the middle one, the flash has been fired before the frame exposure started;
the frame before it is still without any overexposed light.
It is impossible to deduce the exact time in this case.
}

\simplegfx{p}{0.8\textwidth}{flashaudio}
{Trimmed audio recordings of the same moment as in figure \ref{fig:flashtiming}, extracted from the video files, in top-bottom order.
First rows of the frames have been exposed at about $t = 0.03 \text{ s}$.
The flash outputs a distinct ``pop'' sound.
}


The method for measuring the synchronization error can be used to calibrate the differences, which aids in artificial sync with optical flow or other methods.
A similar method using the recorded audio is perhaps more familiar, where a distinct sound in the audio tracks recorded by the cameras would be used for calibration.
%The $t_l$ in equation \ref{eq:timingcalib}

Another method for triggering multiple video recordings is to use a ML-based wired remote trigger by pressing the half-shutter, or focus button when a record key has been configured in ML.
This method did not show any improvements in overcoming the offset.

\subsection{Sample objects}

The mask, human heads, legos, books

\subsection{Hard cases}

Low light? Little texture? Speculars? Hair?

% test X. not feasible for X because Y.

\subsection{Accuracy}

\clearpage

\section{Discussion}

%Perf of cloth animation capture

%other uses - street view, autom driving, geodetic systems

%(corresponding problems in e.g. autonomous driving or harvester machines?)

%(any point in outdoor methods?)


%previous usage in:
%
%	rockstar games / la noire; camera pairs
%
%	polar express / sony imageworks
%
%	ea sports / faro



\subsection{Feasibility}

\subsection{Facial surface motion capture} % {{{

\subsubsection{Deforming skin}

Surface capture of human skin is different from objects that consist of just connected rigid bodies: it stretches and shears in a highly non-predicatable way such that both its local geometry and texture changes, affected by the sub-surface properties of muscles etc.
Traditional methods for tracking rigid objects are thus not viable for high quality.
Joint motion capture has the rigid body assumption.

The deformations can be taken into account with e.g. furukawa etc.

Uncanny valley: humans are really good at identifying faces and correct facial movements; incorrect cases look really wrong.

\subsubsection{Simplifications/assumptions}

Facial expression space, 2d tracking, feature keypoints

Makeup

Facial expression space. Some techiques \cite{faceshift,something} use pre-recorded facial expressions to identify the subject's pose.
They suffer from not being able to accurately describe the temporal changes in finest details, but benefit from densely packed parameterization of facial expressions.
A separate mesh is stored as a three-dimensional template for each expression (such as happy or angry) and each frame is encoded, as a linear combination of these individual expressions.
%Such feature vectors describe well each possible face, and importantly, they eliminate the need for encoding the movement of each vertex, which can be unnecessarily heavy to compute or store.
%The results from this method can easily be mapped to other models than the face of the subject, as it is independent on the actual facial geometry and only uses weights for pre-modeled faces, making it interesting in computer animation.
%
Compare to Facial Action Coding System (FACS) (Hjortsj√∂ 1969), which parameterizes the face in a group of muscular actions. This is similar to grouping vertices in keyframe animation [?].
%
%Expression tracking / 2d capture
%
%Use colors and highpass them; assume uniform lighting and locally uniform texture color (bradley).

\subsection{Future work}

%Although 

Constructing a 3D scanning rig was a large topic on its own.
The field of 3D reconstruction needs content to evolve, even more so for content-specific improvements.
The rig can be used for digitizing real-life objects and studying the process, helping to understand how different algorithms behave on different inputs.
Also, identifying defects in current state-of-the-art is easier when high-resolution test data can be scanned for testing new hypotheses.
%Although readily available multi-view datasets could be used for performance evaluation, not many sets are publicly available.
%the Stanford 3D Scanning Repository (models only)


volumetric reconstruction

octtree storage

octomap

space carving (photo-consistent shapes)

hole filling

visual hull

masking

Mesoscopic level shape reconstruction with texture (bradley)

match moving
